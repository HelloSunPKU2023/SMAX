{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip install torch\n",
    "!pip install transformers # for BERT\n",
    "!pip install pytorch-lightning\n",
    "!pip install 'lightning[extra]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://www.youtube.com/watch?v=vNKIg8rXK6w&t=3959s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.config import *\n",
    "import pandas as pd\n",
    "\n",
    "top_25_products =[\n",
    "    'Petrel',\n",
    "    'RE (Petrel RE, DELFI RE, ECLIPSE, INTERSECT, ODRS, FluidModeler)',\n",
    "    'Delfi Portal',\n",
    "    'Techlog',\n",
    "    'RTDS',\n",
    "    'ProdOps, Avocet, PDF',\n",
    "    'OFM',\n",
    "    'PIPESIM, IAM',\n",
    "    'Deployment',\n",
    "    'OLGA',\n",
    "    'DrillPlan',\n",
    "    'Studio',\n",
    "    'Edge',\n",
    "    'RigHour',\n",
    "    'Storage, File Management, Secure Data Exchange',\n",
    "    'MERAK',\n",
    "    'Omega, VISTA, OMNI3D',\n",
    "    'Symmetry',\n",
    "    'ProSource, InnerLogix',\n",
    "    'PetroMod',\n",
    "    'DrillOps',\n",
    "    'GeoX',\n",
    "    'InterACT Inside',\n",
    "    '3rd Party',\n",
    "    'Data Science'\n",
    "]\n",
    "\n",
    "def get_product_labels(product_name):\n",
    "    labels = [0] * (len(top_25_products)+1)\n",
    "    if product_name in top_25_products:\n",
    "        labels[top_25_products.index(product_name)] = 1\n",
    "    else:\n",
    "        labels[-1] = 1\n",
    "    return labels\n",
    "\n",
    "train_path = f'{DATA_FOLDER_PATH_PROCESSED}/data_train.xlsx'\n",
    "test_path = f'{DATA_FOLDER_PATH_PROCESSED}/data_test.xlsx'\n",
    "\n",
    "config = {\n",
    "    'model_name': 'distilroberta-base',\n",
    "    # 'model_name': 'roberta-base',\n",
    "    'top_n_product': 25,\n",
    "    'text_column': 'Title_Translated',\n",
    "    'label_column': 'Product Name',\n",
    "    'min_title_len': 4,\n",
    "    'max_title_len': 16,\n",
    "    'max_sample': 6000,\n",
    "    'batch_size': 64,\n",
    "    'lr' : 1.5e-6,\n",
    "    'warmup': 0.1,\n",
    "    'w_decay': 0.001,\n",
    "    'train_size': None,\n",
    "    'n_epochs': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SMAX_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, text_column, label_column, tokenizer, max_len, min_len=None, max_sample=None):\n",
    "        self.data_path = data_path\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_len = min_len\n",
    "        self.max_sample = max_sample\n",
    "        self.label_cols = [f'label_{i}' for i in range(25+1)]\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        df = pd.read_excel(self.data_path)\n",
    "        \n",
    "        # Filter out short titles\n",
    "        if self.min_len:\n",
    "            df = df[df[self.text_column].apply(lambda x: len(x.split())) >= self.min_len]\n",
    "        \n",
    "        # Filter out over-sampled data\n",
    "        if self.max_sample:\n",
    "            group_sizes = df[self.label_column].value_counts()\n",
    "            large_groups = group_sizes[group_sizes > self.max_sample].index\n",
    "            df = df.groupby(self.label_column).apply(lambda x: x.sample(n=self.max_sample, random_state=42) if x.name in large_groups else x).reset_index(drop=True)\n",
    "            \n",
    "        # Get product labels by one-hot encoding\n",
    "        if self.label_column in df.columns:\n",
    "            df[self.label_cols] = df[self.label_column].apply(lambda x: pd.Series(get_product_labels(x)))\n",
    "        \n",
    "        self.data = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        labels = torch.FloatTensor(item.loc[self.label_cols].astype('float32').values)\n",
    "        \n",
    "        text = str(item[self.text_column])\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len + 2, # +2 for [CLS] and [SEP]\n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt', \n",
    "            return_attention_mask=True)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].flatten(),\n",
    "            'attention_mask': tokens['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = config['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_dataset = SMAX_Dataset(train_path, config['text_column'], config['label_column'], tokenizer, config['max_title_len'], config['min_title_len'], config['max_sample'])\n",
    "test_dataset = SMAX_Dataset(test_path, config['text_column'], config['label_column'], tokenizer, config['max_title_len'], config['min_title_len'], config['max_sample'])\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class SMAX_DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train_path, test_path, text_column, label_column, max_len, min_len=None, max_sample=None, model_name='distilroberta-base', batch_size=16):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_len = max_len\n",
    "        self.min_len = min_len\n",
    "        self.max_sample = max_sample\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = SMAX_Dataset(self.train_path, self.text_column, self.label_column, self.tokenizer, self.max_len, self.min_len, self.max_sample)\n",
    "            # Split train and val\n",
    "            train_size = int(0.8 * len(self.train_ds))\n",
    "            val_size = len(self.train_ds) - train_size\n",
    "            self.train_ds, self.val_ds = torch.utils.data.random_split(self.train_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "        if stage == 'predict':\n",
    "            self.test_ds = SMAX_Dataset(self.test_path, self.text_column, self.label_column, self.tokenizer, self.max_len, min_len=None, max_sample=None)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "class SMAX_Classifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_classes = config['top_n_product']+1\n",
    "        \n",
    "        self.pretrained_model = AutoModel.from_pretrained(self.config['model_name'], return_dict=True)\n",
    "        \n",
    "        self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
    "\n",
    "        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, self.num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        self.loss_func = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # pretrained layer\n",
    "        output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = torch.mean(output.last_hidden_state, dim=1)\n",
    "        \n",
    "        # calssification layer\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = F.relu(pooled_output)\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.view(-1, self.config['top_n_product']+1), labels.view(-1, self.num_classes))\n",
    "        \n",
    "        return loss, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits = self(**batch)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return {'loss': loss, 'predictions': logits, 'labels': batch['labels']}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits = self(**batch)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True)     \n",
    "        return {'val_loss': loss, 'predictions': logits, 'labels': batch['labels']}\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        _, logits = self(**batch)\n",
    "        return logits\n",
    "    \n",
    "    # https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['w_decay'])\n",
    "        total_steps = self.config['train_size'] / self.config['batch_size'] \n",
    "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Data Module\n",
    "print('Preparing data module...')\n",
    "smax_data_module = SMAX_DataModule(train_path, test_path, config['text_column'], config['label_column'], config['max_title_len'], min_len=config['min_title_len'], max_sample=config['max_sample'], model_name=config['model_name'], batch_size=config['batch_size'])\n",
    "smax_data_module.setup()\n",
    "\n",
    "train_size = len(smax_data_module.train_ds)\n",
    "config['train_size'] = train_size\n",
    "\n",
    "print(f'Train size: {train_size}')\n",
    "\n",
    "# Model \n",
    "smax_model = SMAX_Classifier(config)\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=MODEL_FOLDER_PATH,\n",
    "    filename='smax-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['n_epochs'], \n",
    "    num_sanity_val_steps=50,\n",
    "    callbacks=[\n",
    "        checkpoint_callback, \n",
    "        early_stop_callback\n",
    "        ],\n",
    "    )\n",
    "trainer.fit(smax_model, smax_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# method to convert list of comments into predictions for each comment\n",
    "def classify_raw_comments(model, dm):\n",
    "    predictions = trainer.predict(model, datamodule=dm)\n",
    "    flattened_predictions = np.stack([torch.sigmoid(torch.Tensor(p)) for batch in predictions for p in batch])\n",
    "    return flattened_predictions\n",
    "predictions = classify_raw_comments(smax_model, smax_data_module)\n",
    "\n",
    "test_data = pd.read_excel(test_path)\n",
    "label_column = config['label_column']\n",
    "\n",
    "# print the confusion matrix\n",
    "pred_true = test_data[label_column]\n",
    "pred = [top_25_products[i] if i<25 else 'Others' for i in np.argmax(predictions, axis=1)]\n",
    "\n",
    "label_cols = top_25_products + ['Others']\n",
    "test_data[label_cols] = test_data[label_column].apply(lambda x: pd.Series(get_product_labels(x)))\n",
    "true_labels = np.array(test_data[label_cols])\n",
    "\n",
    "print(metrics.classification_report(pred_true, pred))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(32, 16))\n",
    "for i, attribute in enumerate(label_cols):\n",
    "    fpr, tpr, _ = metrics.roc_curve(\n",
    "        true_labels[:,i].astype(int), predictions[:, i])\n",
    "    auc = metrics.roc_auc_score(\n",
    "        true_labels[:,i].astype(int), predictions[:, i])\n",
    "    plt.plot(fpr, tpr, label='%s %g' % (attribute, auc))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(f\"{config['model_name']} trained on SMAX Datatset - AUC ROC\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "smax_data_module = SMAX_DataModule(train_path, test_path, config['text_column'], config['label_column'], config['max_title_len'], min_len=config['min_title_len'], max_sample=config['max_sample'], model_name=config['model_name'], batch_size=config['batch_size'])\n",
    "smax_data_module.setup()\n",
    "smax_train_ds = smax_data_module.train_ds\n",
    "smax_test_ds = smax_data_module.test_ds\n",
    "\n",
    "\n",
    "print(smax_train_ds.__len__())\n",
    "print(smax_test_ds.__len__())\n",
    "\n",
    "idx=100\n",
    "input_ids = smax_train_ds.__getitem__(idx)['input_ids']\n",
    "attention_mask = smax_train_ds.__getitem__(idx)['attention_mask']\n",
    "labels = smax_train_ds.__getitem__(idx)['labels']\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "print(labels)\n",
    "\n",
    "loss, output = smax_model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), labels.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
