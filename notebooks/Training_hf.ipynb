{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705b514e",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://huggingface.co/transformers/v3.3.1/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b937898",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfc6d8-5b16-46c7-9c7d-20d869a41ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:32:43.980824Z",
     "start_time": "2023-10-16T09:32:42.963982Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.config import *\n",
    "from src.helper_visualization import *\n",
    "from src.helper_pred import *\n",
    "from src.helper_pipeline import *\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3858b",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d43cb-9c69-4ee3-af91-93d06408057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONGRATULATIONS, ready to go ðŸš€\n",
    "\n",
    "# DATA PREPARATION\n",
    "TITLE_WORDS_MIN = 4             # the minimum number of words in the title\n",
    "TITLE_WORDS_MAX = 15            # the maximum number of words in the title\n",
    "TEXT_COL = 'Title_Translated'   # the text column to be used for training\n",
    "TARGET_COL = 'Product Name'     # the target column to be used for training\n",
    "FILER_COL = 'Length'            # the filter column to be used for training\n",
    "PRODUCT_SIZE_MAX_TRAIN = 4000   # the maximum number of samples for each product in training set to balance the data\n",
    "PRODUCT_SIZE_MAX_TEST = None    # the maximum number of samples for each product in test set to balance the data\n",
    "TOP_N_PRODUCTS = 25             # the top n products to be used for training, the rest will be lumped into 'Others'\n",
    "PRODUCT_OTHERS = f'Other Products (not in Top {TOP_N_PRODUCTS})'  # the name of the 'Others' product\n",
    "\n",
    "# TRAINING\n",
    "# ref: https://huggingface.co/transformers/v3.3.1/pretrained_models.html\n",
    "# BERT_MODEL = 'bert-base-uncased' # 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n",
    "CLASS_WEIGHT_FACTOR = 3         # the times of the largest class as the weight of the minor classes. set to 1 to disable class weight\n",
    "\n",
    "BERT_MODEL = 'distilbert-base-uncased' # 6-layer, 768-hidden, 12-heads, 66M parameters. Trained on lower-cased English text.\n",
    "MAX_EPOCH = 50\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "BATCH_SIZE = 64\n",
    "FRACTION = 1\n",
    "# CONGRATULATIONS, ready to go ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cee41b",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'df_train' not in locals():\n",
    "    excel_file_train = f'{DATA_FOLDER_PATH_PROCESSED}/data_train.xlsx'\n",
    "    df_train = pd.read_excel(excel_file_train)\n",
    "print(f'df_train has \\033[94m{df_train.shape[0]}\\033[0m records, memory usage: \\033[94m{df_train.memory_usage().sum()//(1024*1024)}\\033[0mMB')\n",
    "\n",
    "hist_by_labels(df_train, FILER_COL, log=False, left=TITLE_WORDS_MIN-.5, right=TITLE_WORDS_MAX+.5)\n",
    "hist_by_labels(df_train, 'Product Name', log=True, right=TOP_N_PRODUCTS-.5)\n",
    "df_train.sample(10, random_state=42)\n",
    "\n",
    "# Get the Product Name list of Top N products\n",
    "def get_top_n_products(df, target_col, n):\n",
    "    df_target = df[target_col].value_counts().to_frame().reset_index()\n",
    "    df_target.columns = [target_col, 'count']\n",
    "    df_target = df_target.sort_values(by='count', ascending=False)\n",
    "    df_target = df_target.head(n)\n",
    "    return df_target[target_col].tolist()\n",
    "\n",
    "top_n_products = get_top_n_products(df_train, TARGET_COL, TOP_N_PRODUCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf62150",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532d10c",
   "metadata": {},
   "source": [
    "Define the data preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scikit-learn pipeline to remove the title with less than 3 words or more than 20 words\n",
    "pipleline_data_prep_train = Pipeline([\n",
    "    ('title_length_filter', TitleLengthFilter(filter_name=FILER_COL, min_words=TITLE_WORDS_MIN, max_words=TITLE_WORDS_MAX)),\n",
    "    ('other_products_combiner', OtherProductsCombiner(top_products=top_n_products, target_col=TARGET_COL, product_name=PRODUCT_OTHERS)),\n",
    "    ('sample_capper', SampleCapper(max_samples=PRODUCT_SIZE_MAX_TRAIN, target_col=TARGET_COL)),\n",
    "    ('text_lower', TextLower(text_col=TEXT_COL))\n",
    "])\n",
    "\n",
    "pipleline_data_prep_test = Pipeline([\n",
    "    ('title_length_filter', TitleLengthFilter(filter_name=FILER_COL, min_words=TITLE_WORDS_MIN, max_words=TITLE_WORDS_MAX)),\n",
    "    ('other_products_combiner', OtherProductsCombiner(top_products=top_n_products, target_col=TARGET_COL, product_name=PRODUCT_OTHERS)),\n",
    "    ('sample_capper', SampleCapper(max_samples=PRODUCT_SIZE_MAX_TEST, target_col=TARGET_COL)),\n",
    "    ('text_lower', TextLower(text_col=TEXT_COL))\n",
    "])\n",
    "\n",
    "display(pipleline_data_prep_train)\n",
    "display(pipleline_data_prep_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e0c2f",
   "metadata": {},
   "source": [
    "Prepare data for the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode the target labels\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Train data\n",
    "df_train_processed = pipleline_data_prep_train.fit_transform(df_train)\n",
    "df_train_processed = df_train_processed.reset_index(drop=True)\n",
    "df_train_processed = df_train_processed.rename(columns={TEXT_COL: 'text'})\n",
    "df_train_processed['label'] = le.fit_transform(df_train_processed[TARGET_COL])\n",
    "\n",
    "print(f'Train data has \\033[94m{df_train_processed.shape[0]}\\033[0m records, memory usage: \\033[94m{df_train_processed.memory_usage().sum()//(1024*1024)}\\033[0m MB')\n",
    "display(df_train_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e422d05",
   "metadata": {},
   "source": [
    "# Hugging Face Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c80e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Hugging Face transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import evaluate     # helper functions to used in trainer callback to compute accuracy, precision, recall, f1 during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4c963",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], \n",
    "                    max_length=TITLE_WORDS_MAX,\n",
    "                    padding='max_length', \n",
    "                    truncation=True)\n",
    "\n",
    "# Split train data into train and eval\n",
    "train_data = df_train_processed[['text', 'label']]\n",
    "train_data, eval_data = train_test_split(train_data, test_size=0.15, stratify=train_data['label'], random_state=42)\n",
    "\n",
    "# Sample the train and eval data to speed up the training\n",
    "train_data = train_data.sample(frac=FRACTION, random_state=42).reset_index(drop=True)\n",
    "eval_data = eval_data.sample(frac=FRACTION, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Get the number of classes\n",
    "n_classes = len(train_data.label.unique())\n",
    "\n",
    "# Class weighting\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=train_data.label.unique(), y=train_data.label)\n",
    "class_weight_max = np.sqrt(class_weights.min())*CLASS_WEIGHT_FACTOR\n",
    "class_weights = {i: min(np.sqrt(class_weights[i]), class_weight_max) for i in range(len(class_weights))}\n",
    "plot_class_weights(class_weights)\n",
    "\n",
    "hg_train_data = Dataset.from_pandas(train_data)\n",
    "hg_eval_data = Dataset.from_pandas(eval_data)\n",
    "\n",
    "# Tokenize the train and eval data\n",
    "hg_train_data_tokenized = hg_train_data.map(tokenize)\n",
    "hg_eval_data_tokenized = hg_eval_data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=n_classes)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=MAX_EPOCH,                 # total number of training epochs\n",
    "    output_dir='./results',                     # output directory\n",
    "    logging_dir='./logs',                       # directory for storing logs\n",
    "    logging_strategy='epoch',                   # log every epoch\n",
    "    logging_steps=100,                          # log every 100 steps\n",
    "    warmup_steps=500,                           # number of warmup steps for learning rate scheduler\n",
    "    per_device_train_batch_size=BATCH_SIZE,     # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,      # batch size for evaluation\n",
    "    learning_rate=5e-6,                         # learning rate\n",
    "    seed=42,                                    # seed for reproducibility\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define the compute_metrics function to compute the accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metric = evaluate.load('accuracy')\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(model.device))\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Create the Trainer instance to train the model\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                            # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                     # training arguments, defined above\n",
    "    train_dataset=hg_train_data_tokenized,  # training dataset\n",
    "    eval_dataset=hg_eval_data_tokenized,    # evaluation dataset\n",
    "    compute_metrics=compute_metrics,        # the callback that computes metrics of interest\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)])\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Configuration: Classify top \\033[94m{TOP_N_PRODUCTS}\\033[0m products; Text column: \\033[94m{TEXT_COL}\\033[0m, Title words length: (\\033[94m{TITLE_WORDS_MIN}\\033[0m, \\033[94m{TITLE_WORDS_MAX}\\033[0m); Records/product caped at \\033[94m{PRODUCT_SIZE_MAX_TRAIN}\\033[0m')\n",
    "print(f'\\033[94m{FRACTION*100:.1f}%\\033[0m data used - Train_data has \\033[94m{train_data.shape[0]}\\033[0m records; Eval_data has \\033[94m{eval_data.shape[0]}\\033[0m records')\n",
    "print(f'Using \\033[94m{BERT_MODEL}\\033[0m model, Epoch=\\033[94m{MAX_EPOCH}\\033[0m, Early Stop Patience=\\033[94m{EARLY_STOP_PATIENCE}\\033[0m, Batch Size=\\033[94m{BATCH_SIZE}\\033[0m.')\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Plot the training history and perform evaluation on the eval dataset\n",
    "plot_transformer_training_history(trainer)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42cc86",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "if 'df_test' not in locals():\n",
    "    excel_file_test = f'{DATA_FOLDER_PATH_PROCESSED}/data_test.xlsx'\n",
    "    df_test = pd.read_excel(excel_file_test)\n",
    "\n",
    "df_test_processed = pipleline_data_prep_test.fit_transform(df_test)\n",
    "df_test_processed = df_test_processed.reset_index(drop=True)\n",
    "df_test_processed = df_test_processed.rename(columns={TEXT_COL: 'text'})\n",
    "df_test_processed['label'] = le.transform(df_test_processed[TARGET_COL])\n",
    "display(df_test_processed.head())\n",
    "\n",
    "# evaluate on test data\n",
    "test_data = df_test_processed[['text', 'label']]\n",
    "display(test_data.head())\n",
    "print(f'Test data has \\033[94m{test_data.shape[0]}\\033[0m records.')\n",
    "\n",
    "hg_test_data = Dataset.from_pandas(test_data)\n",
    "hg_test_data_tokenized = hg_test_data.map(tokenize)\n",
    "\n",
    "y_actual = hg_test_data_tokenized['label']\n",
    "y_pred = trainer.predict(hg_test_data_tokenized)\n",
    "y_pred = np.argmax(y_pred.predictions, axis=-1)\n",
    "\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "print(f'Accuracy: \\033[94m{accuracy:.4f}\\033[0m')\n",
    "\n",
    "report = classification_report(y_actual, y_pred, digits=3, target_names=le.classes_)\n",
    "print(report)\n",
    "\n",
    "y_actual_decoded = le.inverse_transform(y_actual)\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "plot_confusion_matrix(y_actual_decoded, y_pred_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537443c3",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# train the word2vec_model using additional X_train data\n",
    "# word2vec_model.build_vocab(X_train, update=True)\n",
    "word2vec_model.train(X_train, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "# Define a function to encode the text using Word2Vec\n",
    "def encode_text(text):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            vector = word2vec_model[word]\n",
    "            vectors.append(vector)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim) # Return a vector of zeros if no words are found\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0) # Return the mean of the word vectors\n",
    "\n",
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_word2vec = [encode_text(text) for text in X_train]\n",
    "X_test_word2vec = [encode_text(text) for text in X_test]\n",
    "\n",
    "\n",
    "# Clean up the study if it exists\n",
    "study_name = 'sgd_classifier_word2vec'\n",
    "storage_name = 'sqlite:///optuna_study.db'\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define an objective function to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "\n",
    "    pamams = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-3, log=True),\n",
    "        'eta0': trial.suggest_float('eta0', 1e-3, 1e-1, log=True),\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'adaptive']), #\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 42\n",
    "        }\n",
    "    \n",
    "    # Create and train the SGD Classifier with suggested hyperparameters\n",
    "    sgd_classifier = SGDClassifier(**pamams)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    scores = cross_val_score(sgd_classifier, X_train_word2vec, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10, \n",
    "    n_jobs=-1, \n",
    "    show_progress_bar=True\n",
    "    )  # You can adjust the number of trials\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "best_params = study.best_params\n",
    "best_sore = study.best_value\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_sore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_word2vec = [encode_text(text) for text in X_train]\n",
    "X_test_word2vec = [encode_text(text) for text in X_test]\n",
    "\n",
    "\n",
    "# Clean up the study if it exists\n",
    "study_name = 'sgd_classifier_word2vec'\n",
    "storage_name = 'sqlite:///optuna_study.db'\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define an objective function to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "\n",
    "    pamams = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-3, log=True),\n",
    "        'eta0': trial.suggest_float('eta0', 1e-3, 1e-1, log=True),\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'adaptive']), #\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 42\n",
    "        }\n",
    "    \n",
    "    # Create and train the SGD Classifier with suggested hyperparameters\n",
    "    sgd_classifier = SGDClassifier(**pamams)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    scores = cross_val_score(sgd_classifier, X_train_word2vec, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10, \n",
    "    n_jobs=-1, \n",
    "    show_progress_bar=True\n",
    "    )  # You can adjust the number of trials\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "best_params = study.best_params\n",
    "best_sore = study.best_value\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_sore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'{MODEL_FOLDER_PATH}/word2vec/GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe embeddings\n",
    "embeddings_index = {}\n",
    "dim = 300\n",
    "\n",
    "with open(f'{MODEL_FOLDER_PATH}/glove/glove.6B.{dim}d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Define a function to encode the text using GloVe\n",
    "def encode_text(text):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in embeddings_index:\n",
    "            vector = embeddings_index[word]\n",
    "            vectors.append(vector)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim) # Return a vector of zeros if no words are found\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0) # Return the mean of the word vectors\n",
    "\n",
    "# Vectorize the training and testing data using GloVe\n",
    "X_train_glove = [encode_text(text) for text in X_train]\n",
    "X_test_glove = [encode_text(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330da71",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('KerasNLP version:', keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_SAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = NUM_TRAIN_SAMPLES * TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 3\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# encode the target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3226d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "NUM_CLASSES = len(df_train['Product Name'].unique())\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset, sequence_length=160, name=\"preprocessor_4_tweets\")\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset, preprocessor = preprocessor, num_classes=NUM_CLASSES)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "classifier.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=5e-5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Fit the model.\n",
    "history = classifier.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "y_pred_tf = classifier.predict(X_test)\n",
    "y_pred_tf = np.argmax(y_pred_tf, axis=1)\n",
    "\n",
    "# Decode the predictions\n",
    "y_pred_tf = label_encoder.inverse_transform(y_pred_tf)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_tf)\n",
    "print(f'Accuracy: \\033[94m{accuracy:4f}\\033[0m')\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "print(classification_report(y_test, y_pred_tf, digits=3))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_tf, title='Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
