{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b937898",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfc6d8-5b16-46c7-9c7d-20d869a41ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:32:43.980824Z",
     "start_time": "2023-10-16T09:32:42.963982Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.config import *\n",
    "from src.helper_visualization import *\n",
    "from src.helper_pred import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cee41b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d43cb-9c69-4ee3-af91-93d06408057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_cleaned.xlsx'\n",
    "excel_file_train = f'{DATA_FOLDER_PATH_PROCESSED}/data_train.xlsx'\n",
    "df_train = pd.read_excel(excel_file_train)\n",
    "excel_file_test = f'{DATA_FOLDER_PATH_PROCESSED}/data_test.xlsx'\n",
    "df_test = pd.read_excel(excel_file_test)\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_test.info())\n",
    "hist_by_labels(df_train, 'Length', log=False, left=3.5, right=15.5)\n",
    "hist_by_labels(df_train, 'Product Name', log=True, right=25.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf62150",
   "metadata": {},
   "source": [
    "# Train/Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766bf34",
   "metadata": {},
   "source": [
    "Pipeline Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "TITLE_WORDS_MIN = 3\n",
    "TITLE_WORDS_MAX = 20\n",
    "LONGTAIL_CUTOFF = 200\n",
    "\n",
    "TEXT_COL = 'Title_Enhanced'\n",
    "TARGET_COL = 'Product Name'\n",
    "\n",
    "PRODUCT_SIZE_MAX_TRAIN = 2000\n",
    "PRODUCT_SIZE_MAX_TEST = 300\n",
    "MAX_FEATURES = 20000\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# create a scikit-learn transformer to remove the title with less than 3 words or more than 20 words\n",
    "class TitleLengthFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_words=TITLE_WORDS_MIN, max_words=TITLE_WORDS_MAX):\n",
    "        self.min_words = min_words\n",
    "        self.max_words = max_words\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df = df[df['Length'] >= self.min_words]\n",
    "        df = df[df['Length'] <= self.max_words]\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "# create a scikit-learn transformer to combine the products which have less than 200 samples into one product\n",
    "class LongTailCombiner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_samples=200, target_col=TARGET_COL):\n",
    "        self.min_samples = min_samples\n",
    "        self.target_col = target_col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        counts = df[self.target_col].value_counts()\n",
    "        long_tails = counts.index[counts < self.min_samples]\n",
    "        mask = df[self.target_col].isin(long_tails)\n",
    "        df.loc[mask, self.target_col]='Long Tail'\n",
    "        return df\n",
    "\n",
    "# create a scikit-learn transformer to cap the number of samples for each product\n",
    "class SampleCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_samples=200, target_col=TARGET_COL):\n",
    "        self.max_samples = max_samples\n",
    "        self.target_col = target_col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        counts = df[self.target_col].value_counts()\n",
    "        over_sampled = counts.index[counts > self.max_samples]\n",
    "        # mask = df[self.target_col].isin(over_sampled)\n",
    "        # df = df.drop(df[mask].sample(frac=1-self.max_samples/len(df)).index)\n",
    "        for item in over_sampled:\n",
    "            size = len(df[df[self.target_col]==item])\n",
    "            df = df.drop(df[df[self.target_col]==item].sample(frac=1-self.max_samples/size).index)\n",
    "        return df\n",
    "\n",
    "# create a scikit-learn pipeline to remove the title with less than 3 words or more than 20 words\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipleline_data_prep_train = Pipeline([\n",
    "    ('title_length_filter', TitleLengthFilter(min_words=TITLE_WORDS_MIN, max_words=TITLE_WORDS_MAX)),\n",
    "    ('long_tail_product_combiner', LongTailCombiner(min_samples=LONGTAIL_CUTOFF, target_col=TARGET_COL)),\n",
    "    ('sample_capper', SampleCapper(max_samples=PRODUCT_SIZE_MAX_TRAIN, target_col=TARGET_COL)),\n",
    "    # ('text_vectorizer', TextVectorizer(vectorizer=vectorizer))\n",
    "])\n",
    "\n",
    "pipleline_data_prep_test = Pipeline([\n",
    "    ('title_length_filter', TitleLengthFilter(min_words=TITLE_WORDS_MIN, max_words=TITLE_WORDS_MAX)),\n",
    "    ('long_tail_product_combiner', LongTailCombiner(min_samples=LONGTAIL_CUTOFF*12//100, target_col=TARGET_COL)),\n",
    "    ('sample_capper', SampleCapper(max_samples=PRODUCT_SIZE_MAX_TEST, target_col=TARGET_COL)),\n",
    "    # ('text_vectorizer', TextVectorizer(vectorizer=vectorizer))\n",
    "])\n",
    "display(pipleline_data_prep_train)\n",
    "display(pipleline_data_prep_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70996bb7",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pipleline_data_prep_train.fit_transform(df_train)\n",
    "hist_by_labels(train_data, 'Product Name', log=True, horizontal=True)\n",
    "train_data.info()\n",
    "\n",
    "test_data = pipleline_data_prep_test.fit_transform(df_test)\n",
    "hist_by_labels(test_data, 'Product Name', log=True, horizontal=True)\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537443c3",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# train the word2vec_model using additional X_train data\n",
    "# word2vec_model.build_vocab(X_train, update=True)\n",
    "word2vec_model.train(X_train, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "# Define a function to encode the text using Word2Vec\n",
    "def encode_text(text):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            vector = word2vec_model[word]\n",
    "            vectors.append(vector)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim) # Return a vector of zeros if no words are found\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0) # Return the mean of the word vectors\n",
    "\n",
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_word2vec = [encode_text(text) for text in X_train]\n",
    "X_test_word2vec = [encode_text(text) for text in X_test]\n",
    "\n",
    "\n",
    "# Clean up the study if it exists\n",
    "study_name = 'sgd_classifier_word2vec'\n",
    "storage_name = 'sqlite:///optuna_study.db'\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define an objective function to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "\n",
    "    pamams = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-3, log=True),\n",
    "        'eta0': trial.suggest_float('eta0', 1e-3, 1e-1, log=True),\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'adaptive']), #\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 42\n",
    "        }\n",
    "    \n",
    "    # Create and train the SGD Classifier with suggested hyperparameters\n",
    "    sgd_classifier = SGDClassifier(**pamams)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    scores = cross_val_score(sgd_classifier, X_train_word2vec, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10, \n",
    "    n_jobs=-1, \n",
    "    show_progress_bar=True\n",
    "    )  # You can adjust the number of trials\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "best_params = study.best_params\n",
    "best_sore = study.best_value\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_sore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize the training and testing data using Word2Vec\n",
    "X_train_word2vec = [encode_text(text) for text in X_train]\n",
    "X_test_word2vec = [encode_text(text) for text in X_test]\n",
    "\n",
    "\n",
    "# Clean up the study if it exists\n",
    "study_name = 'sgd_classifier_word2vec'\n",
    "storage_name = 'sqlite:///optuna_study.db'\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Define an objective function to optimize\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "\n",
    "    pamams = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-3, log=True),\n",
    "        'eta0': trial.suggest_float('eta0', 1e-3, 1e-1, log=True),\n",
    "        'loss': trial.suggest_categorical('loss', ['log_loss', 'modified_huber']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'adaptive']), #\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 42\n",
    "        }\n",
    "    \n",
    "    # Create and train the SGD Classifier with suggested hyperparameters\n",
    "    sgd_classifier = SGDClassifier(**pamams)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    scores = cross_val_score(sgd_classifier, X_train_word2vec, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10, \n",
    "    n_jobs=-1, \n",
    "    show_progress_bar=True\n",
    "    )  # You can adjust the number of trials\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "best_params = study.best_params\n",
    "best_sore = study.best_value\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_sore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'{MODEL_FOLDER_PATH}/word2vec/GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe embeddings\n",
    "embeddings_index = {}\n",
    "dim = 300\n",
    "\n",
    "with open(f'{MODEL_FOLDER_PATH}/glove/glove.6B.{dim}d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Define a function to encode the text using GloVe\n",
    "def encode_text(text):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in embeddings_index:\n",
    "            vector = embeddings_index[word]\n",
    "            vectors.append(vector)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim) # Return a vector of zeros if no words are found\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0) # Return the mean of the word vectors\n",
    "\n",
    "# Vectorize the training and testing data using GloVe\n",
    "X_train_glove = [encode_text(text) for text in X_train]\n",
    "X_test_glove = [encode_text(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330da71",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('KerasNLP version:', keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ab21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_SAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = NUM_TRAIN_SAMPLES * TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 3\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# encode the target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3226d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "NUM_CLASSES = len(df_train['Product Name'].unique())\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset, sequence_length=160, name=\"preprocessor_4_tweets\")\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset, preprocessor = preprocessor, num_classes=NUM_CLASSES)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "classifier.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=5e-5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Fit the model.\n",
    "history = classifier.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "y_pred_tf = classifier.predict(X_test)\n",
    "y_pred_tf = np.argmax(y_pred_tf, axis=1)\n",
    "\n",
    "# Decode the predictions\n",
    "y_pred_tf = label_encoder.inverse_transform(y_pred_tf)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_tf)\n",
    "print(f'Accuracy: \\033[94m{accuracy:4f}\\033[0m')\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "print(classification_report(y_test, y_pred_tf, digits=3))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_tf, title='Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
