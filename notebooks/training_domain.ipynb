{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.config import *\n",
    "from src.helper_entity import *\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Specific Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'COMPANY': ['eni', 'petronas', 'tpao', 'slb', 'cvx', 'equinor', 'Ecopetrol', 'omv', 'int', 'ongc', \n",
    "                'bp', 'bsp', 'spic', 'chevron', \"mpcl\", 'schlumberger', 'santos', 'woodside'],\n",
    "    \n",
    "    'GEOUNIT': ['USL', 'SCA', 'SLR', 'KSA', 'ING', 'EUR', 'EAG', 'APG', 'CHG'],\n",
    "    \n",
    "    'PRODUCT': ['Agile Reservoir Modeling', 'Avocet', \n",
    "                'Cameron Supplier Document Management', 'Cameron Supplier Portal', 'Cameron Surface Surveillance', 'ConnectedProduction', \n",
    "                'DELFI RE', 'DNS Management', \n",
    "                'Data Delivery Services', 'Data Ingestion', 'Data Integration Framework', 'Data Integrator', 'Data Migration', 'Data Science', 'Dataiku DSS', \n",
    "                'Delfi Help', 'Delfi Opportunity Assessor', 'Delfi Portal', 'Delfi Production Chemical', 'Developer Portal', 'Delfi',\n",
    "                'DrillOps', 'DrillPlan', 'Drillbench', 'Drilling Insights', 'Drilling Office', 'Drilling Interpretation',\n",
    "                'ECLIPSE', 'EXP_PS', 'Edge', 'Engine Ecosystem', \n",
    "                'Enterprise Data Management Agent', 'Enterprise Data Solution', 'Enterprise Data Workspace', 'Enterprise Developer Portal', 'Enterprise Portal', \n",
    "                'ExplorePlan', 'eSearch',\n",
    "                'FDPlan', 'FORGAS', 'Facility Planner', 'Flaresim', 'FluidModeler', \n",
    "                'GAIA', 'GeoX', \n",
    "                'INTERSECT', 'InnerLogix', 'Integrated Asset Modeler', 'InterACT', \n",
    "                'Kinetix'\n",
    "                'LiveQuest', \n",
    "                'MEPO', 'MERAK', 'Malcom', \n",
    "                'Nasuni', 'NetApp ANF', \n",
    "                'OFM', 'OLGA', 'OLGA Online', 'OMNI3D', 'Ocean Framework', 'Ocean Plug-ins','Ocean Store','Omega', 'On Demand Reservoir Simulation', 'Osprey', \n",
    "                'PIPEFLO', 'PIPESIM', 'PIPESYS', 'PerformView', 'Petrel', 'Petrel Exploration Geology', 'Petrel RE', 'PetroMod', \n",
    "                'Petrotechnical Suite', 'ProSource', 'ProcessOps', 'ProdOps', 'Production Data Foundation', 'Provisioning & Decommissioning', \n",
    "                'RP Planner', 'RTDS', 'Rapid Screening', 'Reservoir Analytics', 'RigHour', \n",
    "                'Seabed', 'Secure Data Exchange', 'Simulation Cluster Manager', 'Studio', 'Symmetry', 'Spotfire', \n",
    "                'TGX', 'TDI', 'Techlog', \n",
    "                'VISAGE', 'VISTA', \n",
    "                'WELLFLO', 'WMS', 'Wellbarrier', 'WinGLink', \n",
    "                'ZFS', \n",
    "                ]}\n",
    "\n",
    "def export_entity_csv():\n",
    "# load Entities into a dataframe which has two columns: 'Entity' and 'Label'\n",
    "    import pandas as pd\n",
    "    array_entities = []\n",
    "\n",
    "    for key in Entities.keys():\n",
    "        for entity in Entities[key]:\n",
    "            # insert a row into the dataframe\n",
    "            array_entities.append([key, entity])\n",
    "    df_entities = pd.DataFrame(array_entities, columns=['Label', 'Entity'])\n",
    "\n",
    "    # export the dataframe to cvs file\n",
    "    # print current working directory\n",
    "    from src.config import SRC_FOLDER_PATH\n",
    "    \n",
    "    df_entities.to_csv(f'{SRC_FOLDER_PATH}/entities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/prem2017/new-entity-labelling/blob/master/new_entity_labelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get ReGex pattern from a text which ignores the case and matches the whole word\n",
    "def get_regex_pattern(text):\n",
    "    return rf\"(?i)\\b{text}\\b\"\n",
    "\n",
    "# export_entity_csv() # export the entities to csv if not already done\n",
    "if 'df_consolidated' not in locals():\n",
    "    data_url = f'{DATA_FOLDER_PATH_PROCESSED}/data_consolidated.xlsx'\n",
    "    df_consolidated = pd.read_excel(data_url)\n",
    "    \n",
    "df_samples = df_consolidated.sample(2000, random_state=42)\n",
    "# export_entity_csv()\n",
    "df_entities = pd.read_csv(f'{DATA_FOLDER_PATH_PROCESSED}/entities.csv')\n",
    "\n",
    "# prepare the training data\n",
    "training_data = []\n",
    "# List of entities with overlapping mentions\n",
    "entity_list = df_entities['Entity'].tolist()\n",
    "\n",
    "# Sorting the entities based on their length\n",
    "sorted_entities = sorted(entity_list, key=len, reverse=True)\n",
    "\n",
    "for index, row in tqdm(df_samples.iterrows(), total=len(df_samples), desc=\"Processing data\"):\n",
    "    text = row['Title_Translated']\n",
    "    entities = []\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        pattern = get_regex_pattern(entity)\n",
    "        # Check for the pattern in the text\n",
    "        if re.search(pattern, text):\n",
    "            label = df_entities[df_entities['Entity'] == entity]['Label'].iloc[0]  # Get label for the entity\n",
    "            start_index = re.search(pattern, text).start()\n",
    "            end_index = start_index + len(entity)\n",
    "            entities.append((start_index, end_index, label))\n",
    "    \n",
    "    training_data.append((text, {'entities': entities}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Labelling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Set up the pipeline for training\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# test the trained model before training\n",
    "text = 'exxonmobil to invest $300 million in india to expand refining capacity'\n",
    "doc = nlp(text)\n",
    "print(\"Entities in '%s'\" % text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ents = ner.move_names\n",
    "print('[Existing Entities] = ', ner.move_names)\n",
    "\n",
    "# Add the new label to ner\n",
    "ner.add_label(\"PRODUCT\")\n",
    "# ner.add_label(\"GEOUNIT\")\n",
    "\n",
    "new_ents = ner.move_names\n",
    "# print('\\n[All Entities] = ', ner.move_names)\n",
    "\n",
    "print('\\n\\n[New Entities] = ', list(set(new_ents) - set(prev_ents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Train the model\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    # Training for 30 iterations\n",
    "    for iteration in tqdm(range(10), desc=\"Training\"):\n",
    "        # use the pretrained model to update the pipeline\n",
    "        optimizer = nlp.resume_training()\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example = []\n",
    "            # Update the model with iterating each text\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "                example.append(Example.from_dict(doc, gold))\n",
    "            nlp.update(example, losses=losses, drop=0.0, sgd=optimizer)\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained model after training\n",
    "text = 'exxonmobil to invest $300 million in india to expand refining capacity using Petrel software'\n",
    "doc = nlp(text)\n",
    "print(\"Entities in '%s'\" % text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pretrained model to update the pipeline\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "pipe_exceptions = [\"ner\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Start the training process with the new label\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    for _ in range(3):  # Adjust the number of iterations as needed\n",
    "        for text, annotations in tqdm(training_data, total = len(training_data), desc=\"Training model\"):\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "            optimizer.update([example], losses=losses, drop=0.3)\n",
    "\n",
    "# # train NER for 30 iterations\n",
    "# losses = {}\n",
    "\n",
    "# for batch in tqdm(minibatch(training_data, size=2), total=len(df_samples)//2, desc=\"Training model\"):\n",
    "#     for text, annotations in batch:\n",
    "#         # create Example\n",
    "#         doc = nlp.make_doc(text)\n",
    "#         example = Example.from_dict(doc, annotations)\n",
    "#         # Update the model\n",
    "#         nlp.update([example], losses=losses, drop=0.3)\n",
    "    # print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained model\n",
    "for text, _ in training_data:\n",
    "    doc = nlp(text)\n",
    "    if doc.ents:\n",
    "        #print the text in blue\n",
    "        print(f'\\033[1;34;40m {text}\\033[0m')\n",
    "        #print the entities found if the label is not PRODUCT\n",
    "        print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    # print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
