{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a748876-39e2-4b69-bda4-76b36e17f4cd",
   "metadata": {},
   "source": [
    "# Init Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593929d",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40677a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note of the module installations\n",
    "# install python with pyenv (ref: https://github.com/lewagon/data-setup/blob/e5a239926f304d452704718136b6e2f017c7303d/macOS.md#installing-python-with-pyenv)\n",
    "# setup virtual environment with pyenv (ref:https://github.com/lewagon/data-setup/blob/e5a239926f304d452704718136b6e2f017c7303d/macOS.md#installing-python-with-pyenv\n",
    "# error: 'Failed to activate virtualenv' (ref:https://github.com/pyenv/pyenv-virtualenv/issues/387)\n",
    "# pip install fasttext (0.9.2) (after installing the dependencies: numpy, scipy, pybind11, setuptools, wheel)\n",
    "\n",
    "# install Jupyter notebook and nbextensions (ref: https://github.com/lewagon/data-setup/blob/e5a239926f304d452704718136b6e2f017c7303d/macOS.md#jupyter-notebook-extensions)\n",
    "#   pip install jupyter_contrib_nbextensions\n",
    "#   pip install --upgrade notebook==6.4.12 (ref:https://stackoverflow.com/questions/49647705/jupyter-nbextensions-does-not-appear)\n",
    "#   pip install traitlets==5.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b937898",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfc6d8-5b16-46c7-9c7d-20d869a41ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-16T09:32:43.980824Z",
     "start_time": "2023-10-16T09:32:42.963982Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.config import *\n",
    "from src.helper_visualization import *\n",
    "from src.helper_text import *\n",
    "from src.helper_langID import *\n",
    "from src.helper_translation import *\n",
    "from src.helper_pred import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2ebf9-1e49-48b3-9989-ea190f96d532",
   "metadata": {},
   "source": [
    "# Combine Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of file paths that match the pattern\n",
    "files = glob.glob(f'{DATA_FOLDER_PATH_RAW}/data_202*.xlsx')\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file and read it into a DataFrame\n",
    "for file in files:\n",
    "    df = pd.read_excel(file, index_col=None)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames into one\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965d317-641f-4700-b460-d36276bbcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "df_combined.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2692b71-70f9-4567-a213-a003b05f085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns which have large number of null values will be dropped \n",
    "columns_to_drop = [\n",
    "    'Escalated To Engineering', \n",
    "    'Bug Type', \n",
    "    'Status Reason', \n",
    "    'Escalated to L2',\n",
    "    'Category',\n",
    "    'Completion Code'\n",
    "]\n",
    "try: \n",
    "    df_combined.drop(columns_to_drop, axis=1, inplace = True)\n",
    "    df_combined.dropna(subset=['Request ID'], inplace=True)\n",
    "    df_combined.dropna(subset=['Product Name'], inplace=True)\n",
    "    df_combined.dropna(subset=['Title'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# surprisingly, there are over 100k duplications\n",
    "df_combined.drop_duplicates(subset=['Title', 'Product Name'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426248b-15ce-4d37-a870-a69b17c67d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name and path\n",
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_combined.xlsx'\n",
    "\n",
    "# Export the DataFrame to Excel\n",
    "df_combined.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c2d47-3a46-4ccf-8d26-625ce2dca9b0",
   "metadata": {},
   "source": [
    "# Load Data for Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9700383-1461-49d3-86c4-4989b4a8817f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-16T09:33:28.968Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'df_combined' not in locals():\n",
    "    data_url = f'{DATA_FOLDER_PATH_PROCESSED}/data_combined.xlsx'\n",
    "    df_combined = pd.read_excel(data_url, index_col=None)\n",
    "print(df_combined.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd684eca-5c11-4f08-b9f7-41e62f596f3a",
   "metadata": {},
   "source": [
    "# Clean-up the Title Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c91b8",
   "metadata": {},
   "source": [
    "## Quick Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063ed32-3753-44c6-84f0-b3a50abf3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_combined.copy()\n",
    "\n",
    "# Delete unnecessary columns, add needed columns\n",
    "df_processed.drop(columns=[\n",
    "    'Created Time', \n",
    "    'Customer Company', 'Customer Country', \n",
    "    'Priority', 'Urgency', 'Impact', \n",
    "    'Service Definition', 'Service Desk Group', 'Status',\n",
    "    'Closed Time', \n",
    "    'Response Time (Min)', 'Resolution Time (Min)', \n",
    "    'Contracts Reference', 'Creation Source'\n",
    "    ], inplace=True)\n",
    "\n",
    "# Add new columns\n",
    "df_processed['Title_Processed']=pd.NA\n",
    "df_processed['Language']=pd.NA\n",
    "df_processed['Length'] = 0\n",
    "df_processed['Title_Translated']=pd.NA\n",
    "df_processed['Title_Enhanced']=pd.NA\n",
    "df_processed['Tags']=pd.NA\n",
    "\n",
    "# Run quick clean up on the 'Title' column and save the result to the 'Title_Processed' column\n",
    "\n",
    "# Set the number of rows to process\n",
    "num = df_processed['Title'].notnull().sum()\n",
    "\n",
    "# Initialize a progress bar with the total number of rows\n",
    "progress_bar = tqdm(total=num, desc=\"Processing Rows\", unit=\" row\")\n",
    "\n",
    "# Function to process a single row and update the 'Processed_Title' column\n",
    "def process_row(index):\n",
    "    # processed_title = preprocess_step_1(df_combined.at[index, 'Title'])\n",
    "    processed_title = quick_clean_up(df_processed.at[index, 'Title'])\n",
    "    df_processed.at[index, 'Title_Processed'] = processed_title\n",
    "    df_processed.at[index, 'Length'] = count_words(processed_title)\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Define the number of parallel workers (adjust this based on your CPU cores)\n",
    "num_workers = 8\n",
    "\n",
    "# Create a ThreadPoolExecutor with the specified number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use the executor to process rows in parallel\n",
    "    executor.map(process_row, df_processed.index)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Delete records with missing values in 'Title_Processed' columns\n",
    "df_processed.dropna(subset=['Title_Processed'], inplace=True)\n",
    "\n",
    "# Remove duplicates based on 'Title_Processed' and 'ProductName' columns\n",
    "df_processed.drop_duplicates(subset=['Title_Processed', 'Product Name'], keep='first', inplace=True)\n",
    "\n",
    "print(df_processed.info())\n",
    "hist_by_labels(df_processed, 'Length', log=False)\n",
    "\n",
    "df_processed.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846983c-24de-4541-b3ea-6a710ad7ed3d",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427ff81-089a-4939-8b2c-6aed1ddbe219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run language detection on the 'Title_Processed' column and save the result to the 'Language' column\n",
    "# This is the step required to run bulk language translation using Google Translate API for performance reason\n",
    "# The language detection is done using FastText library since FastText is better than the langdetect library and other options.\n",
    "# ref: https://medium.com/besedo-engineering/language-identification-for-very-short-texts-a-review-c9f2756773ad\n",
    "# FastText has difficulty detecting CJK languages mixed with English, so we will use udf-8 encoding to detect CJK languages directly.\n",
    "\n",
    "# Set the number of rows to process\n",
    "num = df_processed['Title_Processed'].notnull().sum()\n",
    "\n",
    "# df['Title_Translated'] = df['Title_Processed']\n",
    "# Initialize a progress bar with the total number of rows\n",
    "progress_bar = tqdm(total=num, desc=\"Processing Rows\", unit=\" row\")\n",
    "\n",
    "# Function to process a single row and update the 'Processed_Title' column\n",
    "def process_row(index):\n",
    "    df_processed.at[index, 'Language'] = detect_language_fasttext(df_processed.at[index, 'Title_Processed'])\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Define the number of parallel workers (adjust this based on your CPU cores)\n",
    "num_workers = 8\n",
    "\n",
    "# Create a ThreadPoolExecutor with the specified number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use the executor to process rows in parallel\n",
    "    executor.map(process_row, df_processed.index)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "hist_by_labels(df_processed, \"Language\", log=True, horizontal=True, left=10.5)\n",
    "\n",
    "print(f\"Language detected with low confidence: \\033[94m{df_processed['Language'].value_counts(normalize=True)['unknown'] * 100:.3f}%\\033[0m.\")\n",
    "df_processed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8b44c-7e70-44e7-af70-e729bf0fe747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the top 10 languages and drop the rest and the unknown. \n",
    "# why? There are high chances the detected language were not correct due to wrong spelling, etc.\n",
    "language_counts = df_processed['Language'].value_counts()\n",
    "cutoff = 90\n",
    "language_others = language_counts.index[language_counts < cutoff]\n",
    "\n",
    "mask_others = df_processed['Language'].isin(language_others)\n",
    "df_processed.loc[mask_others,'Language']='unknown'\n",
    "\n",
    "# Drop rows where 'Language' is equal to 'unknown'\n",
    "df_detection_unknown = df_processed[df_processed['Language'] == 'unknown']\n",
    "df_processed = df_processed[df_processed['Language'] != 'unknown']\n",
    "\n",
    "print(f\"Language detected with low confidence: \\033[94m{df_detection_unknown.shape[0]}\\033[0m.\")\n",
    "print(df_processed.info())\n",
    "print(df_detection_unknown.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820dda9-d136-4da6-8022-1459bd7b1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to Excel for future steps without having to run the previous steps again\n",
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_processed.xlsx'\n",
    "df_processed.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file}\\033[0m saved')\n",
    "\n",
    "excel_file_unknown = f'{DATA_FOLDER_PATH_PROCESSED}/data_langID_unknown.xlsx'\n",
    "df_detection_unknown.to_excel(excel_file_unknown, index=False)\n",
    "print(f'\\033[94m{excel_file_unknown}\\033[0m saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_detection_unknown.head(20))\n",
    "hist_by_labels(df_detection_unknown, \"Length\", log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92575d-7fd5-4e8b-9e23-d67726c396d3",
   "metadata": {},
   "source": [
    "## Translation of Non-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd9679-e4cf-47d2-a77e-32e972bd5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the 'Title_Processed' column and save the result to the 'Title_Translated' column\n",
    "# language by language to avoid Google Translate API quota limit\n",
    "# skip English obviously\n",
    "# the length limit is 1250 characters due to Google Translate API limit for CJK languages\n",
    "# load the processed data df_processed if not loaded yet\n",
    "if 'df_processed' not in locals():\n",
    "    excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_processed.xlsx'\n",
    "    df_processed = pd.read_excel(excel_file)\n",
    "\n",
    "df_translated = df_processed.copy()\n",
    "df_translated['Title_Translated'] = df_translated['Title_Processed']\n",
    "\n",
    "grouped = df_translated.groupby(\"Language\")\n",
    "\n",
    "# Initialize tqdm to display progress bar\n",
    "pbar = tqdm(total=len(grouped), desc=f\"Translating\")\n",
    "\n",
    "# Create an empty list to store the processed groups\n",
    "processed_groups = []\n",
    "\n",
    "# Iterate through sub DataFrames\n",
    "for lang, group in grouped:\n",
    "    pbar.set_description(f\"Processing [\\033[94m{lang}/{len(group)}\\033[0m]\")\n",
    "\n",
    "    translated_titles = translate_array(\n",
    "        group[\"Title_Translated\"].tolist(),\n",
    "        src_lang=lang,\n",
    "        tar_lang='en',\n",
    "        length_limit=1250)\n",
    "    group[\"Title_Translated\"] = translated_titles  # Update \"Title_Translated\" column in the group\n",
    "    \n",
    "    processed_groups.append(group)\n",
    "    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Concatenate the groups back into a single DataFrame\n",
    "df_translated = pd.concat(processed_groups, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated.head(-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ebaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records with missing values in 'Title_Translated' columns\n",
    "# there should be no missing values in 'Title_Translated' columns but just in case\n",
    "df_translated.dropna(subset=['Title_Translated'], inplace=True)\n",
    "\n",
    "# remove the confusion between Petrel and Petrel RE\n",
    "df_translated['Title_Translated'] = df_translated['Title_Translated'].str.replace('petrel re', 'PetrelRE', case=False)\n",
    "\n",
    "# Remove duplicates based on 'Title_Translated' and 'Product Name' columns\n",
    "df_translated.drop_duplicates(subset=['Title_Translated', 'Product Name'], keep='first', inplace=True)\n",
    "\n",
    "print(df_translated.info())\n",
    "df_translated.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab544da-93a1-4c0a-86a6-b72e12a69354",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_translated.xlsx'\n",
    "df_translated.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file}\\033[0m saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600e1e0",
   "metadata": {},
   "source": [
    "## Enhance Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9256a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_translated' not in locals():\n",
    "    excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_translated.xlsx'\n",
    "    df_translated = pd.read_excel(excel_file)\n",
    "\n",
    "df_enhanced = df_translated.copy()\n",
    "# Set the number of rows to process\n",
    "num = df_enhanced['Title_Translated'].notnull().sum()\n",
    "df_enhanced['Title_Enhanced'] = \"\"\n",
    "# Initialize a progress bar with the total number of rows\n",
    "progress_bar = tqdm(total=num, desc=\"Processing Rows\", unit=\" row\")\n",
    "\n",
    "# Function to process a single row and update the 'Processed_Title' column\n",
    "def process_row(index):\n",
    "    df_enhanced.at[index, 'Title_Enhanced'] = enhance_title(df_enhanced.at[index, 'Title_Translated'])\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Define the number of parallel workers (adjust this based on your CPU cores)\n",
    "num_workers = 8\n",
    "\n",
    "# Create a ThreadPoolExecutor with the specified number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Use the executor to process rows in parallel\n",
    "    executor.map(process_row, df_enhanced.index)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "print(df_enhanced.info())\n",
    "df_translated.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "display(df_enhanced.head(20))\n",
    "\n",
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_enhanced.xlsx'\n",
    "df_enhanced.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file}\\033[0m has been saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3537a-faa7-4f48-95cc-0dfc113b2790",
   "metadata": {},
   "source": [
    "## Extract Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479466a3-ecbf-46a2-bbfe-2ffca4d83b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_enhanced' not in locals():\n",
    "    excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_enhanced.xlsx'\n",
    "    df_enhanced = pd.read_excel(excel_file , dtype={'Request ID': str, 'Title_Cleaned': str})\n",
    "\n",
    "df_cleaned = df_enhanced.copy()\n",
    "print(df_cleaned.info())\n",
    "df_cleaned.head(10)\n",
    "\n",
    "# Set the number of rows to process\n",
    "num = df_cleaned['Title_Enhanced'].notnull().sum()\n",
    "df_cleaned['Tags'] = \"\"\n",
    "# Initialize a progress bar with the total number of rows\n",
    "progress_bar = tqdm(total=num, desc=\"Processing Rows\", unit=\" row\")\n",
    "\n",
    "# Function to process a single row and update the 'Processed_Title' column\n",
    "def process_row(index):\n",
    "    df_cleaned.at[index, 'Tags'] = extract_keywords(df_cleaned.at[index, 'Title_Enhanced'])\n",
    "    df_cleaned.at[index, 'Length'] = count_words(df_cleaned.at[index, 'Title_Enhanced'])\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Define the number of parallel workers (adjust this based on your CPU cores)\n",
    "num_workers = 8\n",
    "\n",
    "# Create a ThreadPoolExecutor with the specified number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Use the executor to process rows in parallel\n",
    "    executor.map(process_row, df_cleaned.index)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Remove duplicates based on 'Title' and 'ProductName' columns after translation\n",
    "df_cleaned.dropna(subset=['Tags'], inplace=True)\n",
    "df_cleaned.drop_duplicates(subset=['Tags', 'Product Name'], keep='first', inplace=True)\n",
    "print(df_cleaned.info())\n",
    "\n",
    "# remove the same title pointing to multiple products\n",
    "n_title = df_cleaned['Tags'].value_counts()\n",
    "good_title = n_title.index[n_title == 1]\n",
    "print (len(good_title))\n",
    "\n",
    "mask = df_cleaned['Tags'].isin(good_title)\n",
    "df_cleaned = df_cleaned[mask]\n",
    "print(df_cleaned.info())\n",
    "print(df_cleaned.isnull().sum().sort_values(ascending = False))\n",
    "display(df_cleaned.head(20))\n",
    "# Specify the file name and path\n",
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_cleaned.xlsx'\n",
    "df_cleaned.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file}\\033[0m has been saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457b308",
   "metadata": {},
   "source": [
    "# Consolidate Product Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee746ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_cleaned' not in locals():\n",
    "    excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_cleaned.xlsx'\n",
    "    df_cleaned = pd.read_excel(excel_file , dtype={'Request ID': str, 'Title_Cleaned': str})\n",
    "print(df_cleaned.info())\n",
    "\n",
    "df_consolidated = df_cleaned.copy()\n",
    "display(df_consolidated.head(-10))\n",
    "\n",
    "dict_category_mapping = {\n",
    "    'Others': ['Other'],\n",
    "    'Studio': ['Ocean Framework for Studio'],\n",
    "    'Techlog': ['Ocean Framework for Techlog'],\n",
    "    'PIPESIM, IAM': ['PIPESIM', 'Integrated Asset Modeler'],\n",
    "    'Omega, VISTA, OMNI3D': ['Omega', 'OMEGA', 'VISTA', 'OMNI3D'],\n",
    "    'ProSource, InnerLogix': ['ProSource', 'EXP_PS', 'InnerLogix'],\n",
    "    'ProdOps, Avocet, PDF': ['ProdOps', 'Production Data Foundation', 'Avocet'],\n",
    "    'Engine Ecosystem, Sim Cluster Mgr.': ['Engine Ecosystem', 'Simulation Cluster Manager'],\n",
    "    'Petrel': ['Petrel Exploration Geology', 'Petrel Project Explorer', 'Ocean Framework for Petrel', 'Ocean Plug-ins for Petrel - SLB'],\n",
    "    'Storage, File Management, Secure Data Exchange': ['Storage', 'Nasuni', 'File Management', 'Secure Data Exchange'],\n",
    "    'Delfi Portal': ['Delfi Portal', 'SAuth', 'License', 'Licensing', 'Environment', 'Authorization', 'VM', 'TGX', 'Remote App'],\n",
    "    'RE (Petrel RE, DELFI RE, ECLIPSE, INTERSECT, ODRS, FluidModeler)': ['Petrel RE', 'DELFI RE', 'ECLIPSE', 'INTERSECT', 'On Demand Reservoir Simulation', 'FluidModeler'],\n",
    "    \n",
    "    '3rd Party': ['Ocean Plug-ins for Petrel - Third party', 'Ocean Plug-ins for Petrel - Third Party',\n",
    "                '3rd party application', 'Third-Party Applications', 'Ocean Plug-ins for Techlog - Third Party', \n",
    "                'App â€“ Third Party', 'App - Third Party'],\n",
    "    \n",
    "    'Deployment': ['Provisioning & Decommissioning', 'Software Demo and Evaluation', 'Internal Deployment', 'Image', \n",
    "                'Deployment Status', \"New PTS Deployment\", 'Remove PTS Deployment', 'System Deployment', 'Deployment',\n",
    "                'Cloud Project Creation', 'Cloud Project Maintenance', 'Tenant Maintenance', 'DNS Management',\n",
    "                'CCM - Contract Management', 'CCM - Catalog Management', 'Software and Solution Quotation']\n",
    "}\n",
    "# Use Module Name to replace Product Name for all 'Petrotechnical Suite - Domain Profiles'\n",
    "\n",
    "df_consolidated.loc[df_consolidated['Product Name'] == 'Petrotechnical Suite - Domain Profiles', 'Product Name'] = df_consolidated['Module Name']\n",
    "df_consolidated.drop(columns=['Module Name'], inplace=True)\n",
    "df_consolidated.dropna(subset=['Product Name'], inplace=True)\n",
    "\n",
    "# combine the following products into corresponding categories\n",
    "for category, products in dict_category_mapping.items():\n",
    "    df_consolidated.loc[df_consolidated['Product Name'].isin(products), 'Product Name'] = category\n",
    "\n",
    "#drop the rows which Product Name is 'Quality and Feedback' and 'Software Training Services'\n",
    "df_consolidated = df_consolidated[df_consolidated['Product Name'] != 'Others']\n",
    "df_consolidated = df_consolidated[df_consolidated['Product Name'] != 'Delfi Help']\n",
    "df_consolidated = df_consolidated[df_consolidated['Product Name'] != 'Quality and Feedback']\n",
    "df_consolidated = df_consolidated[df_consolidated['Product Name'] != 'Software Training Services']\n",
    "\n",
    "#Drop the products which have less than 50 records\n",
    "df_consolidated = df_consolidated.groupby('Product Name').filter(lambda x : len(x)>50)\n",
    "\n",
    "print(df_consolidated.info())\n",
    "\n",
    "hist_by_labels(df_cleaned, 'Product Name', log=True, horizontal=True)\n",
    "hist_by_labels(df_consolidated, 'Product Name', top=None, log=True, horizontal=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and test sets with 90% and 10% respectively of each product\n",
    "df_train, df_test = train_test_split(df_consolidated, test_size=0.1, random_state=42, stratify=df_consolidated['Product Name'])\n",
    "\n",
    "# Export the DataFrame to Excel\n",
    "excel_file_consolidated = f'{DATA_FOLDER_PATH_PROCESSED}/data_consolidated.xlsx'\n",
    "df_consolidated.to_excel(excel_file_consolidated, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file_consolidated}\\033[0m has been saved.')\n",
    "\n",
    "excel_file_train = f'{DATA_FOLDER_PATH_PROCESSED}/data_train.xlsx'\n",
    "df_train.to_excel(excel_file_train, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file_train}\\033[0m has been saved.')\n",
    "\n",
    "excel_file_test = f'{DATA_FOLDER_PATH_PROCESSED}/data_test.xlsx'\n",
    "df_test.to_excel(excel_file_test, index=False)  # Set index to False if you don't want to export the DataFrame index\n",
    "print(f'\\033[94m{excel_file_test}\\033[0m has been saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440953fd",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44346842",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_train.xlsx'\n",
    "\n",
    "df_train = pd.read_excel(excel_file , dtype={'Request ID': str, 'Title_Cleaned': str})\n",
    "print(df_train.info())\n",
    "hist_by_labels(df_train, 'Product Name', top=None, log=True, horizontal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to augment a dataframe with a given product name\n",
    "def augment_product_data(df, target_size=500):\n",
    "\n",
    "    result = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    df_size = len(df)\n",
    "    lang_agent = ['fr', 'ja', 'ru', 'es', 'ko', 'zh', 'pt', 'ar', 'de', 'it']\n",
    "    \n",
    "    num_iterations = min(target_size // df_size-1, len(lang_agent))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        df_temp = df.copy()\n",
    "        translated_titles = df_temp[\"Title_Translated\"].tolist()\n",
    "        translated_titles = translate_array(translated_titles, src_lang='en', tar_lang=lang_agent[i],length_limit=1250)\n",
    "        translated_titles = translate_array(translated_titles, src_lang=lang_agent[i], tar_lang='en',length_limit=1250)\n",
    "\n",
    "        df_temp['Title_Translated'] = translated_titles\n",
    "        result = pd.concat([result, df_temp], ignore_index=True)\n",
    "    return result\n",
    "\n",
    "TARGET_SIZE = 5000\n",
    "df_augmented = pd.DataFrame(columns=df_train.columns)\n",
    "\n",
    "# filter the products which have less than 2500 records\n",
    "grouped = df_train.groupby(\"Product Name\")\n",
    "# grouped = grouped.filter(lambda x : len(x)>2500)\n",
    "\n",
    "# Initialize tqdm to display progress bar\n",
    "pbar = tqdm(total=len(grouped), desc=f\"Processing\")\n",
    "\n",
    "# Create an empty list to store the processed groups\n",
    "processed_groups = []\n",
    "\n",
    "# Iterate through sub DataFrames\n",
    "for product, group in grouped:\n",
    "    if (len(group) > TARGET_SIZE//2):\n",
    "        pbar.update(1)  # Update the progress bar\n",
    "        continue\n",
    "    pbar.set_description(f\"Processing [{product}/{len(group)}]\")\n",
    "    df_product = augment_product_data(group, target_size=TARGET_SIZE)\n",
    "\n",
    "    processed_groups.append(df_product)\n",
    "    pbar.update(1)  # Update the progress bar\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Concatenate the groups back into a single DataFrame\n",
    "df_additional = pd.concat(processed_groups, ignore_index=True)\n",
    "df_augmented = pd.concat([df_augmented, df_additional], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_augmented.copy()\n",
    "\n",
    "df_augmented = pd.concat([df_train, df_augmented], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.info())\n",
    "df_augmented.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented['Title_Cleaned'] = df_augmented['Title_Translated'].apply(final_clean_up)\n",
    "df_augmented.dropna(subset=['Title_Cleaned'], inplace=True)\n",
    "df_augmented['Length'] = df_augmented['Title_Cleaned'].apply(count_words)\n",
    "df_augmented.drop_duplicates(subset=['Title_Cleaned', 'Product Name'], keep='first', inplace=True)\n",
    "df_augmented.drop_duplicates(subset=['Title_Cleaned'], inplace=True)\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_augmented.info())\n",
    "\n",
    "hist_by_labels(df_train, 'Product Name', top=None, log=True, horizontal=True)\n",
    "hist_by_labels(df_augmented, 'Product Name', top=None, log=True, horizontal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the file name and path\n",
    "excel_file = f'{DATA_FOLDER_PATH_PROCESSED}/data_augmented.xlsx'\n",
    "\n",
    "# Export the DataFrame to Excel\n",
    "df_augmented.to_excel(excel_file, index=False)  # Set index to False if you don't want to export the DataFrame index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
